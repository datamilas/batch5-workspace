{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1b6b75485105e36c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# BLU01 - Exercises Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**\n",
    "\n",
    "Attention Windows users: The grader will run on Linux, and using power shell statements will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0240afddd4fae69d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import chardet\n",
    "import hashlib # for grading purposes\n",
    "import math\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-93e0b4e1a40ebaaa",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q1: Use a shell command to count lines\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-05a3d2245d57305d",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the total number of lines of files data/exercises/surfing.txt and data/exercises/body_boarding.txt\n",
    "# and store the result in variable count_total.  \n",
    "# There is more than one way to do this. Hint: concatenate both files.\n",
    "# count_total = ...\n",
    "\n",
    "first_file = !wc -l < data/exercises/surfing.txt\n",
    "second_file = !wc -l < data/exercises/body_boarding.txt\n",
    "count_total = int(first_file[0]) + int(second_file[0])\n",
    "count_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2e69c6137e73d90c",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "expected_hash = '71ee45a3c0db9a9865f7313dd3372cf60dca6479d46261f3542eb9346e4a04d6'\n",
    "assert hashlib.sha256(str(count_total).encode()).hexdigest() == expected_hash, \"count_total is wrong\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now store in 2 variables the first third and the last third of the lines existent in file data/exercises/surfing.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-14cf2619322a8763",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# NOT GRADED optional exercise!!\n",
    "# Add the first third and the last third of the lines to the variables first_third and last_third. \n",
    "# Make it work to any files using count_total_surfing/3 for instance in the bash commands\n",
    "filename = \"./data/exercises/surfing.txt\"\n",
    "count_total_surfing = !wc -l < {filename}\n",
    "first_third = !head -{int(int(count_total_surfing[0])/3)} < {filename}\n",
    "last_third = !tail -{int(int(count_total_surfing[0])/3)} < {filename}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-20d96111430874d4",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Test the first_third and last_third exercise by running the following (ungraded) asserts.\n",
    "assert first_third[-1][0] == 'o'\n",
    "assert last_third[0][0] == 'o'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e27cb8588bb5fd40",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q2: Read a file with specific delimiter\n",
    "\n",
    "Read file **data/exercises/surfing_sessions.csv** into a pandas DataFrame.\n",
    "\n",
    "First, you should preview the file using a shell command in order to find out the used delimiter, and other properties of this file.\n",
    "\n",
    "Then, you should use function read_csv to read the data into a DataFrame. The resulting DataFrame should have the last column as index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-942e0590467e20d9",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surfer|wave_power|location|visit_id\r\n",
      "3|37,6|Rocky Point|146097\r\n",
      "3|5,7|Chuns|146087\r\n",
      "3|5|Noronha(Cacimba)|130395\r\n",
      "3|5|Rocky Point|130363\r\n",
      "3|11,3|Noronha(Conceicao)|130362\r\n",
      "3|8,9|Noronha(Boldo)|130360\r\n",
      "3|32,7|Pupukea|126903\r\n",
      "3|32,7|Pupukea|126902\r\n",
      "3|9,7|Ehukai|126591\r\n"
     ]
    }
   ],
   "source": [
    "# Use a shell command to preview the data\n",
    "\n",
    "!head -10 < data/exercises/surfing_sessions.csv\n",
    "\n",
    "# Use function read_csv to read the data into a DataFrame\n",
    "df2 = pd.read_csv(\"./data/exercises/surfing_sessions.csv\", sep=\"|\", index_col=\"visit_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0c275c9acaa9c74d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert df2.loc[126903, 'location'] == 'Pupukea', \"df2 data is wrong\"\n",
    "assert set(df2.columns) == {'location', 'surfer', 'wave_power'}, \"df2 data is wrong\"\n",
    "assert len(df2) == 108, \"df2 data is wrong\"\n",
    "assert df2.index[0] == 146097, \"df2 data is wrong\"\n",
    "\n",
    "expected_hash = 'a27dd8b953f484c0d74059da102e4b8e513630292de5d17ff5585fc6743d62f0'\n",
    "assert hashlib.sha256(df2.index.name.encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f6f9efef818e4a83",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q3: Read a csv file with problems\n",
    "\n",
    "Read file **data/exercises/surfing_sessions_w_problems.csv** using function `read_csv`. Pay attention to the following:\n",
    "* remove all lines where the number of columns is different from the majority of lines.\n",
    "* use the first column as index\n",
    "* there are some inputs in the file that should be interpreted as NaN, make sure you select the right one when reading the file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2771b707556c08d2",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visit_id;surfer;wave_power;location\r\n",
      "146097;3;37.6;Rocky Point\r\n",
      "146087;3;5.7;Chuns\r\n",
      "130395;3;-1;Noronha(Cacimba)\r\n",
      "130363;3;5;Rocky Point\r\n",
      "130362;3;11.3;Noronha(Conceicao)\r\n",
      "130360;3;8.9;Noronha(Boldo)\r\n",
      "126903;3;32.7;Pupuke\r\n",
      "126902;3;missing-data;Pupukea;;\r\n",
      "126591;3;9.7;Ehukai\r\n",
      "126590;3;10.4;Changes\r\n",
      "126341;3;8.2;Changes\r\n",
      "126340;3;38.6;Laniakea;\r\n",
      "125540;3;34.1;Laniakea\r\n",
      "122199;3;14.4;Glass Doors\r\n",
      "121170;3;34.9;Walter’s West\r\n",
      "120454;3;-1;Gas Chambers\r\n",
      "119736;3;21.7;Rocky Point;\r\n",
      "119735;3;21.2;Rocky Point;;\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 9: expected 4 fields, saw 6\\nSkipping line 13: expected 4 fields, saw 5\\nSkipping line 18: expected 4 fields, saw 5\\nSkipping line 19: expected 4 fields, saw 6\\n'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surfer</th>\n",
       "      <th>wave_power</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visit_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>146097</th>\n",
       "      <td>3</td>\n",
       "      <td>37.6</td>\n",
       "      <td>Rocky Point</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146087</th>\n",
       "      <td>3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>Chuns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130395</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Noronha(Cacimba)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130363</th>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Rocky Point</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130362</th>\n",
       "      <td>3</td>\n",
       "      <td>11.3</td>\n",
       "      <td>Noronha(Conceicao)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130360</th>\n",
       "      <td>3</td>\n",
       "      <td>8.9</td>\n",
       "      <td>Noronha(Boldo)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126903</th>\n",
       "      <td>3</td>\n",
       "      <td>32.7</td>\n",
       "      <td>Pupuke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126591</th>\n",
       "      <td>3</td>\n",
       "      <td>9.7</td>\n",
       "      <td>Ehukai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126590</th>\n",
       "      <td>3</td>\n",
       "      <td>10.4</td>\n",
       "      <td>Changes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126341</th>\n",
       "      <td>3</td>\n",
       "      <td>8.2</td>\n",
       "      <td>Changes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125540</th>\n",
       "      <td>3</td>\n",
       "      <td>34.1</td>\n",
       "      <td>Laniakea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122199</th>\n",
       "      <td>3</td>\n",
       "      <td>14.4</td>\n",
       "      <td>Glass Doors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121170</th>\n",
       "      <td>3</td>\n",
       "      <td>34.9</td>\n",
       "      <td>Walter’s West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120454</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gas Chambers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          surfer  wave_power            location\n",
       "visit_id                                        \n",
       "146097         3        37.6         Rocky Point\n",
       "146087         3         5.7               Chuns\n",
       "130395         3         NaN    Noronha(Cacimba)\n",
       "130363         3         5.0         Rocky Point\n",
       "130362         3        11.3  Noronha(Conceicao)\n",
       "130360         3         8.9      Noronha(Boldo)\n",
       "126903         3        32.7              Pupuke\n",
       "126591         3         9.7              Ehukai\n",
       "126590         3        10.4             Changes\n",
       "126341         3         8.2             Changes\n",
       "125540         3        34.1            Laniakea\n",
       "122199         3        14.4         Glass Doors\n",
       "121170         3        34.9       Walter’s West\n",
       "120454         3         NaN        Gas Chambers"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read file data/exercises/surfing_sessions_w_problems.csv with read_csv\n",
    "# df3 = pd.read_csv(...)\n",
    "\n",
    "!cat< data/exercises/surfing_sessions_w_problems.csv\n",
    "df3 = pd.read_csv(\"data/exercises/surfing_sessions_w_problems.csv\", sep=\";\", error_bad_lines=False,\n",
    "                  index_col=\"visit_id\", na_values={'wave_power': [-1, \"missing-data\"]})\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-601b354802964776",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.isnan(df3.loc[130395, 'wave_power']), \"df3 data is wrong\"\n",
    "assert df3.loc[126903, 'wave_power'] == 32.7, \"df3  data is wrong\"\n",
    "\n",
    "mean_wave_power = df3['wave_power'].mean()\n",
    "assert math.isclose(17.741, mean_wave_power, rel_tol=1e-3), \"df3 data is wrong\"\n",
    "\n",
    "expected_hash = 'a27dd8b953f484c0d74059da102e4b8e513630292de5d17ff5585fc6743d62f0'\n",
    "assert hashlib.sha256(df3.index.name.encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5cfd5a97b1c49a0e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q4: Repair a csv file after importing\n",
    "Read the same file **data/exercises/surfing_sessions_w_problems.csv** using function `csv.reader()`. But now, be sure you don't miss any lines with relevant information! \n",
    "\n",
    "* use csv module to import everything to a list of lists\n",
    "* create a df with only 4 meaningful columns and then \n",
    "* replace garbage values with NaN's \n",
    "* format the columns with the right type\n",
    "* set the index of the df to column `visit_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-119041388e17fb72",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "surfer          int64\n",
       "wave_power    float64\n",
       "location       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read file data/exercises/surfing_sessions_w_problems.csv using csv.reader() and add result to\n",
    "lines = []\n",
    "with open(\"data/exercises/surfing_sessions_w_problems.csv\", newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=';')\n",
    "    for line in reader:\n",
    "        lines.append(line)\n",
    "\n",
    "\n",
    "\n",
    "new_lines = []\n",
    "for line in lines[1:]:\n",
    "    if line[2] == \"-1\" or line[2] == \"missing-data\": line[2]=np.nan\n",
    "    new_lines.append(line[0:4])\n",
    "\n",
    "df4 = pd.DataFrame(columns=lines[0], data = new_lines)\n",
    "\n",
    "# replace invalid values with nan (np.nan)\n",
    "# YOUR CODE HERE\n",
    "# set types per dataframe column (always use int64 when int's are needed)\n",
    "# YOUR CODE HERE\n",
    "# set a new index to the dataframe\n",
    "# YOUR CODE HERE\n",
    "#df4.set_index(\"visit_id\", inplace=True)\n",
    "\n",
    "df4 = df4.astype({'visit_id': 'int64', \"surfer\": \"int64\", \"wave_power\": \"float\"}).set_index(\"visit_id\")\n",
    "df4.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a42299f9047590d5",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.isnan(df4.loc[130395, 'wave_power']), \"df4 content is wrong\"\n",
    "assert df4.loc[126340, 'wave_power'] == 38.6, \"df4 content is wrong\"\n",
    "\n",
    "mean_wave_power = df4['wave_power'].mean()\n",
    "assert math.isclose(19.626, mean_wave_power, rel_tol=1e-3), \"df4 content is wrong\"\n",
    "\n",
    "assert df4['surfer'].dtype == np.int64, \"df4 types are wrong\"\n",
    "assert df4['location'].dtype == object, \"df4 types are wrong\"\n",
    "assert df4['wave_power'].dtype == float, \"df4 types are wrong\"\n",
    "assert df4.index.dtype == np.int64, \"df4 types are wrong\"\n",
    "\n",
    "\n",
    "expected_hash = 'a27dd8b953f484c0d74059da102e4b8e513630292de5d17ff5585fc6743d62f0'\n",
    "assert hashlib.sha256(df4.index.name.encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-389bc42fe462c70e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q5: Read a JSON file\n",
    "\n",
    "Read file **data/exercises/portugal_production_of_electricity_gwh.json**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5c2125479f7e50e5",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Total</th>\n",
       "      <th>Total renewables</th>\n",
       "      <th>Hydropower &gt; 10MW</th>\n",
       "      <th>Hydropower &lt; 10MW</th>\n",
       "      <th>Biomass</th>\n",
       "      <th>Windpower</th>\n",
       "      <th>Geothermal power</th>\n",
       "      <th>Photovoltaic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1995</td>\n",
       "      <td>33264</td>\n",
       "      <td>9501</td>\n",
       "      <td>7962</td>\n",
       "      <td>492</td>\n",
       "      <td>988</td>\n",
       "      <td>16</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1996</td>\n",
       "      <td>34520</td>\n",
       "      <td>15895</td>\n",
       "      <td>14007</td>\n",
       "      <td>658</td>\n",
       "      <td>959</td>\n",
       "      <td>21</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997</td>\n",
       "      <td>34207</td>\n",
       "      <td>14301</td>\n",
       "      <td>12537</td>\n",
       "      <td>638</td>\n",
       "      <td>1036</td>\n",
       "      <td>38</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1998</td>\n",
       "      <td>38984</td>\n",
       "      <td>14224</td>\n",
       "      <td>12488</td>\n",
       "      <td>566</td>\n",
       "      <td>1022</td>\n",
       "      <td>89</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1999</td>\n",
       "      <td>43287</td>\n",
       "      <td>8915</td>\n",
       "      <td>7042</td>\n",
       "      <td>589</td>\n",
       "      <td>1081</td>\n",
       "      <td>122</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2000</td>\n",
       "      <td>43764</td>\n",
       "      <td>13260</td>\n",
       "      <td>11040</td>\n",
       "      <td>675</td>\n",
       "      <td>1296</td>\n",
       "      <td>168</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2001</td>\n",
       "      <td>46509</td>\n",
       "      <td>16083</td>\n",
       "      <td>13605</td>\n",
       "      <td>770</td>\n",
       "      <td>1345</td>\n",
       "      <td>256</td>\n",
       "      <td>105</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2002</td>\n",
       "      <td>46107</td>\n",
       "      <td>10190</td>\n",
       "      <td>7551</td>\n",
       "      <td>706</td>\n",
       "      <td>1473</td>\n",
       "      <td>362</td>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2003</td>\n",
       "      <td>46852</td>\n",
       "      <td>18037</td>\n",
       "      <td>15163</td>\n",
       "      <td>891</td>\n",
       "      <td>1394</td>\n",
       "      <td>496</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2004</td>\n",
       "      <td>45105</td>\n",
       "      <td>12597</td>\n",
       "      <td>9570</td>\n",
       "      <td>577</td>\n",
       "      <td>1547</td>\n",
       "      <td>816</td>\n",
       "      <td>84</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2005</td>\n",
       "      <td>46575</td>\n",
       "      <td>8616</td>\n",
       "      <td>4737</td>\n",
       "      <td>381</td>\n",
       "      <td>1651</td>\n",
       "      <td>1773</td>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2006</td>\n",
       "      <td>49041</td>\n",
       "      <td>16187</td>\n",
       "      <td>10633</td>\n",
       "      <td>834</td>\n",
       "      <td>1704</td>\n",
       "      <td>2926</td>\n",
       "      <td>85</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2007</td>\n",
       "      <td>47253</td>\n",
       "      <td>16593</td>\n",
       "      <td>9927</td>\n",
       "      <td>522</td>\n",
       "      <td>1882</td>\n",
       "      <td>4037</td>\n",
       "      <td>201</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2008</td>\n",
       "      <td>45969</td>\n",
       "      <td>15140</td>\n",
       "      <td>6781</td>\n",
       "      <td>517</td>\n",
       "      <td>1852</td>\n",
       "      <td>5757</td>\n",
       "      <td>192</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2009</td>\n",
       "      <td>50207</td>\n",
       "      <td>19017</td>\n",
       "      <td>8108</td>\n",
       "      <td>901</td>\n",
       "      <td>2087</td>\n",
       "      <td>7577</td>\n",
       "      <td>184</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2010</td>\n",
       "      <td>54093</td>\n",
       "      <td>28755</td>\n",
       "      <td>15459</td>\n",
       "      <td>1088</td>\n",
       "      <td>2614</td>\n",
       "      <td>9182</td>\n",
       "      <td>197</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2011</td>\n",
       "      <td>52465</td>\n",
       "      <td>24691</td>\n",
       "      <td>11294</td>\n",
       "      <td>820</td>\n",
       "      <td>2923</td>\n",
       "      <td>9162</td>\n",
       "      <td>210</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2012</td>\n",
       "      <td>46614</td>\n",
       "      <td>20410</td>\n",
       "      <td>6093</td>\n",
       "      <td>567</td>\n",
       "      <td>2951</td>\n",
       "      <td>10260</td>\n",
       "      <td>146</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2013</td>\n",
       "      <td>51673</td>\n",
       "      <td>30610</td>\n",
       "      <td>13701</td>\n",
       "      <td>1167</td>\n",
       "      <td>3051</td>\n",
       "      <td>12015</td>\n",
       "      <td>197</td>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2014</td>\n",
       "      <td>52802</td>\n",
       "      <td>32405</td>\n",
       "      <td>15071</td>\n",
       "      <td>1341</td>\n",
       "      <td>3049</td>\n",
       "      <td>12111</td>\n",
       "      <td>205</td>\n",
       "      <td>627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2015</td>\n",
       "      <td>52423</td>\n",
       "      <td>25514</td>\n",
       "      <td>9048</td>\n",
       "      <td>752</td>\n",
       "      <td>3104</td>\n",
       "      <td>11608</td>\n",
       "      <td>204</td>\n",
       "      <td>799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2016</td>\n",
       "      <td>60279</td>\n",
       "      <td>33448</td>\n",
       "      <td>15689</td>\n",
       "      <td>1221</td>\n",
       "      <td>3070</td>\n",
       "      <td>12474</td>\n",
       "      <td>172</td>\n",
       "      <td>822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2017</td>\n",
       "      <td>59432</td>\n",
       "      <td>24309</td>\n",
       "      <td>7009</td>\n",
       "      <td>623</td>\n",
       "      <td>3220</td>\n",
       "      <td>12248</td>\n",
       "      <td>217</td>\n",
       "      <td>992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Year  Total  Total renewables  Hydropower > 10MW  Hydropower < 10MW  \\\n",
       "0   1995  33264              9501               7962                492   \n",
       "1   1996  34520             15895              14007                658   \n",
       "2   1997  34207             14301              12537                638   \n",
       "3   1998  38984             14224              12488                566   \n",
       "4   1999  43287              8915               7042                589   \n",
       "5   2000  43764             13260              11040                675   \n",
       "6   2001  46509             16083              13605                770   \n",
       "7   2002  46107             10190               7551                706   \n",
       "8   2003  46852             18037              15163                891   \n",
       "9   2004  45105             12597               9570                577   \n",
       "10  2005  46575              8616               4737                381   \n",
       "11  2006  49041             16187              10633                834   \n",
       "12  2007  47253             16593               9927                522   \n",
       "13  2008  45969             15140               6781                517   \n",
       "14  2009  50207             19017               8108                901   \n",
       "15  2010  54093             28755              15459               1088   \n",
       "16  2011  52465             24691              11294                820   \n",
       "17  2012  46614             20410               6093                567   \n",
       "18  2013  51673             30610              13701               1167   \n",
       "19  2014  52802             32405              15071               1341   \n",
       "20  2015  52423             25514               9048                752   \n",
       "21  2016  60279             33448              15689               1221   \n",
       "22  2017  59432             24309               7009                623   \n",
       "\n",
       "    Biomass  Windpower  Geothermal power  Photovoltaic  \n",
       "0       988         16                42             1  \n",
       "1       959         21                49             1  \n",
       "2      1036         38                51             1  \n",
       "3      1022         89                58             1  \n",
       "4      1081        122                80             1  \n",
       "5      1296        168                80             1  \n",
       "6      1345        256               105             2  \n",
       "7      1473        362                96             2  \n",
       "8      1394        496                90             3  \n",
       "9      1547        816                84             3  \n",
       "10     1651       1773                71             3  \n",
       "11     1704       2926                85             5  \n",
       "12     1882       4037               201            24  \n",
       "13     1852       5757               192            41  \n",
       "14     2087       7577               184           160  \n",
       "15     2614       9182               197           215  \n",
       "16     2923       9162               210           282  \n",
       "17     2951      10260               146           393  \n",
       "18     3051      12015               197           479  \n",
       "19     3049      12111               205           627  \n",
       "20     3104      11608               204           799  \n",
       "21     3070      12474               172           822  \n",
       "22     3220      12248               217           992  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read file data/exercises/portugal_production_of_electricity_gwh.json with read_json\n",
    "# df5 = read_json(...)\n",
    "filename=\"data/exercises/portugal_production_of_electricity_gwh.json\"\n",
    "#!cat < {filename}\n",
    "\n",
    "df5 = pd.read_json(filename,orient=\"table\")\n",
    "df5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3f085fa2cdad9319",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(df5) == 23\n",
    "assert set(df5.columns) == {'Biomass', 'Geothermal power', 'Hydropower < 10MW', 'Hydropower > 10MW', \n",
    "                           'Photovoltaic', 'Total','Total renewables','Windpower','Year'}, \"df5 columns are wrong\"\n",
    "\n",
    "expected_hash = '48beaa4bb16f0656bb4e4d3abb5da6ff6a50b52be037354f8c33455a7534102e'\n",
    "assert hashlib.sha256(str(df5.loc[:,'Hydropower > 10MW'].sum()).encode()).hexdigest() == expected_hash, \"df5 content is wrong\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4d4b970f55b3e427",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q6: Read an Excel file\n",
    "\n",
    "Read file **data/exercises/portugal_gas_emissions_per_year.xlsx** using function read_excel. Pay attention to the following:\n",
    "\n",
    "* you should grab the table \"Series\" in sheet \"metadata\"\n",
    "* use column 'Serie' as index\n",
    "* make sure you keep only the rows and columns with data\n",
    "* set the variable distinct_scales with the number of ... distinct scales found in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-017a7c31b0ddc38e",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Read file data/exercises/portugal_gas_emissions_per_year.xlsx with read_excel\n",
    "\n",
    "df6 = pd.read_excel(\"data/exercises/portugal_gas_emissions_per_year.xlsx\", sheet_name=\"metadata\", skiprows=21, skipfooter=6, header=1, index_col=\"Serie\")\n",
    "\n",
    "\n",
    "distinct_scales = len(df6.Scale.unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-91d5f07e40585680",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert distinct_scales == 2\n",
    "assert isinstance(df6, pd.DataFrame)\n",
    "expected_hash = '60c3ad36f77e7366103fe36a3f551a0cde7d64e26d3102ddb8b953d6208a6006'\n",
    "assert hashlib.sha256(\n",
    "        df6.loc[\n",
    "            df6.index==\"Nitrogen oxides\", \n",
    "            \"Measure Unit\"][0].encode()\n",
    "    ).hexdigest() == expected_hash, \"df6 is wrong\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4a7407517d4d92c3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q7: Find the encoding of a file\n",
    "\n",
    "Find the encoding used in file **data/exercises/cities.csv**, using the method that was shown in the Learning Units.\n",
    "\n",
    "Then, read the data into a DataFrame, using the read_csv method and find the `City` characters that has distance equal to 2377."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bd6944fd63bb38d8",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Find the encoding of file data/exercises/cities.csv\n",
    "filename=\"data/exercises/cities.csv\"\n",
    "#!cat < {filename}\n",
    "\n",
    "encoding = chardet.detect(open(filename, 'rb').read())[\"encoding\"]\n",
    "\n",
    "# Read the file into a DataFrame\n",
    "df7 = pd.read_csv(filename, encoding=encoding)\n",
    "df7[df7.distance==2377].City\n",
    "\n",
    "# Find the name of the city with distance = 2377\n",
    "city_found = df7[df7.distance==2377].City.reset_index().City[0]\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-687fb83c97e03355",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(df7, pd.DataFrame)\n",
    "\n",
    "expected_hash_1 = '969aef39a1d4cb1c5928c774cd7a4e3ccfc064a18fbd43a70193a2631d8a122d'\n",
    "assert hashlib.sha256(encoding.encode()).hexdigest() == expected_hash_1, \"encoding is wrong\"\n",
    "\n",
    "expected_hash_2 = '22522b6940ec2bcc42ea98f55878bee105c27254c4340fbce9fbc951b5aed078'\n",
    "assert hashlib.sha256(city_found.encode()).hexdigest() == expected_hash_2, \"city_found is wrong\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d103dfae3d5e11fe",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q8: Import a Random Sample of a Big File\n",
    "\n",
    "Consider the file **data/exercises/world_percentage_of_literacy.tsv**. Let's imagine this file is really huge, with a lot of rows!  Read the file using a random sample of 10 rows. Count the actual lines with `wc` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fb39107093dd73fb",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Literacy rate (all)</th>\n",
       "      <th>Male literacy</th>\n",
       "      <th>Female literacy</th>\n",
       "      <th>Gender difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Estonia</td>\n",
       "      <td>99.8%</td>\n",
       "      <td>99.8%</td>\n",
       "      <td>99.8%</td>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Korea, Democratic People's Republic of</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lesotho</td>\n",
       "      <td>79.4%</td>\n",
       "      <td>70.1%</td>\n",
       "      <td>88.3%</td>\n",
       "      <td>-18.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Luxembourg</td>\n",
       "      <td>not reported by UNESCO 2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mexico</td>\n",
       "      <td>94.4%</td>\n",
       "      <td>95.6%</td>\n",
       "      <td>93.3%</td>\n",
       "      <td>2.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Monaco</td>\n",
       "      <td>not reported by UNESCO 2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>92.6%</td>\n",
       "      <td>93.6%</td>\n",
       "      <td>91.7%</td>\n",
       "      <td>1.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>not reported by UNESCO 2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Zambia</td>\n",
       "      <td>63.4%</td>\n",
       "      <td>70.9%</td>\n",
       "      <td>56.0%</td>\n",
       "      <td>14.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>86.5%</td>\n",
       "      <td>88.5%</td>\n",
       "      <td>84.6%</td>\n",
       "      <td>4.0%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Country          Literacy rate (all)  \\\n",
       "0                                 Estonia                        99.8%   \n",
       "1  Korea, Democratic People's Republic of                       100.0%   \n",
       "2                                 Lesotho                        79.4%   \n",
       "3                              Luxembourg  not reported by UNESCO 2015   \n",
       "4                                  Mexico                        94.4%   \n",
       "5                                  Monaco  not reported by UNESCO 2015   \n",
       "6                               Sri Lanka                        92.6%   \n",
       "7                             Switzerland  not reported by UNESCO 2015   \n",
       "8                                  Zambia                        63.4%   \n",
       "9                                Zimbabwe                        86.5%   \n",
       "\n",
       "  Male literacy Female literacy Gender difference  \n",
       "0         99.8%           99.8%              0.0%  \n",
       "1        100.0%          100.0%              0.0%  \n",
       "2         70.1%           88.3%            -18.2%  \n",
       "3           NaN             NaN               NaN  \n",
       "4         95.6%           93.3%              2.2%  \n",
       "5           NaN             NaN               NaN  \n",
       "6         93.6%           91.7%              1.9%  \n",
       "7           NaN             NaN               NaN  \n",
       "8         70.9%           56.0%             14.9%  \n",
       "9         88.5%           84.6%              4.0%  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read file data/exercises/world_percentage_of_literacy.tsv with wc and save the number of lines\n",
    "# in lines_in_file (as an integer), using \n",
    "filename = \"data/exercises/world_percentage_of_literacy.tsv\"\n",
    "\n",
    "#!cat <{filename}\n",
    "lines_in_file = !wc -l < {filename}\n",
    "lines_in_file = int(lines_in_file[0])-1\n",
    "\n",
    "\n",
    "# make parameter rows_to_skip equal to the lines you want to skip loading \n",
    "# don't forget: 10 rows should be fecthed\n",
    "\n",
    "sample_number = 10\n",
    "n_rows_to_skip = lines_in_file - sample_number\n",
    "\n",
    "rows_to_skip = random.sample(range(1, lines_in_file-1), n_rows_to_skip)\n",
    "\n",
    "df8 = pd.read_csv(filename, sep=\"\\t\", skiprows=rows_to_skip)\n",
    "df8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ec15a8be42137a01",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# getting remaining list of countries, removing rows_to_skip from the file\n",
    "df_helper = pd.read_csv( \n",
    "    'data/exercises/world_percentage_of_literacy.tsv', \n",
    "    sep='\\t',\n",
    "    header=0 \n",
    ")\n",
    "\n",
    "total_indexes = list(df_helper.index)\n",
    "for s in rows_to_skip:\n",
    "    total_indexes.remove(s-1)\n",
    "\n",
    "expected_hash = '7559ca4a957c8c82ba04781cd66a68d6022229fca0e8e88d8e487c96ee4446d0'\n",
    "assert hashlib.sha256(str(lines_in_file).encode()).hexdigest() == expected_hash, \"lines_in_file are wrong\"\n",
    "assert isinstance(df8, pd.DataFrame)\n",
    "assert list(df8['Country'])==list(df_helper.iloc[total_indexes]['Country'])\n",
    "assert df8.shape[0]==10, \"df8 size is wrong\"\n",
    "\n",
    "expected_hash = '4a44dc15364204a80fe80e9039455cc1608281820fe2b24f1e5233ade6af1dd5'\n",
    "assert  hashlib.sha256(str(df8.shape[0]).encode()).hexdigest() == expected_hash\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f87e07a7b2cacd9d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q9: Loading a Big File\n",
    "\n",
    "Read file **data/exercises/world_percentage_of_literacy.tsv** using chunks keep only the columns `Country` and `Gender difference`.\n",
    "Note that:\n",
    "* file should be read by chunks of 20 countries\n",
    "* the missing values should be removed (filtered in each chunk)\n",
    "* the `Gender difference` should be converted to type float (in each chunk)\n",
    "* the index should be incremental starting from 0 (i.e, you don't need to read any column as the index)\n",
    "\n",
    "At the end, calculate the average `Gender difference`\n",
    "\n",
    "Tip: Be sure you investigate the data you are about to load before doing any code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5106c25a705296e8",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Literacy rate (all)</th>\n",
       "      <th>Male literacy</th>\n",
       "      <th>Female literacy</th>\n",
       "      <th>Gender difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>World</td>\n",
       "      <td>86.3%</td>\n",
       "      <td>90.0%</td>\n",
       "      <td>82.7%</td>\n",
       "      <td>7.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>38.2%</td>\n",
       "      <td>52.0%</td>\n",
       "      <td>24.2%</td>\n",
       "      <td>27.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Albania</td>\n",
       "      <td>97.6%</td>\n",
       "      <td>98.4%</td>\n",
       "      <td>96.8%</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>80.2%</td>\n",
       "      <td>87.2%</td>\n",
       "      <td>73.1%</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Angola</td>\n",
       "      <td>71.1%</td>\n",
       "      <td>82.0%</td>\n",
       "      <td>60.7%</td>\n",
       "      <td>21.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>Venezuela</td>\n",
       "      <td>95.4%</td>\n",
       "      <td>95.0%</td>\n",
       "      <td>95.7%</td>\n",
       "      <td>-0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>Vietnam</td>\n",
       "      <td>94.5%</td>\n",
       "      <td>96.3%</td>\n",
       "      <td>92.8%</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Yemen</td>\n",
       "      <td>70.1%</td>\n",
       "      <td>85.1%</td>\n",
       "      <td>55.0%</td>\n",
       "      <td>30.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Zambia</td>\n",
       "      <td>63.4%</td>\n",
       "      <td>70.9%</td>\n",
       "      <td>56.0%</td>\n",
       "      <td>14.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>86.5%</td>\n",
       "      <td>88.5%</td>\n",
       "      <td>84.6%</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Country Literacy rate (all) Male literacy Female literacy  \\\n",
       "0          World               86.3%         90.0%           82.7%   \n",
       "1    Afghanistan               38.2%         52.0%           24.2%   \n",
       "2        Albania               97.6%         98.4%           96.8%   \n",
       "3        Algeria               80.2%         87.2%           73.1%   \n",
       "5         Angola               71.1%         82.0%           60.7%   \n",
       "..           ...                 ...           ...             ...   \n",
       "189    Venezuela               95.4%         95.0%           95.7%   \n",
       "190      Vietnam               94.5%         96.3%           92.8%   \n",
       "191        Yemen               70.1%         85.1%           55.0%   \n",
       "192       Zambia               63.4%         70.9%           56.0%   \n",
       "193     Zimbabwe               86.5%         88.5%           84.6%   \n",
       "\n",
       "     Gender difference  \n",
       "0                  7.3  \n",
       "1                 27.8  \n",
       "2                  1.6  \n",
       "3                 14.0  \n",
       "5                 21.3  \n",
       "..                 ...  \n",
       "189               -0.7  \n",
       "190                3.4  \n",
       "191               30.1  \n",
       "192               14.9  \n",
       "193                4.0  \n",
       "\n",
       "[151 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read file data/exercises/world_percentage_of_literacy.tsv\n",
    "filepath = \"data/exercises/world_percentage_of_literacy.tsv\"\n",
    "#!head -10 < {filepath}\n",
    "\n",
    "# the chunks should be appended in a list called chunk_arr\n",
    "\n",
    "chunks_iter = pd.read_csv(\n",
    "    filepath,\n",
    "    sep=\"\\t\",\n",
    "    chunksize=20\n",
    ")\n",
    "\n",
    "def filter_data(data):\n",
    "    data.dropna(inplace=True)\n",
    "    data[\"Gender difference\"] = data[\"Gender difference\"].str.replace('%', '').astype(\"float\")\n",
    "    return data\n",
    "\n",
    "chunk_arr = []\n",
    "for data_chunk in chunks_iter:\n",
    "    data_chunk_filtered = filter_data(data_chunk)\n",
    "    chunk_arr.append(data_chunk_filtered)\n",
    "\n",
    "df9 = pd.concat(chunk_arr, axis=0)\n",
    "lit_avg = df9[\"Gender difference\"].mean()\n",
    "\n",
    "df9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f60a22c3465d5b83",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "expected_hash = 'cc63c8d6f0f9a8260d40b5c9a62d4fce5b693bab5750c90fd1f038ff64e37d6d'\n",
    "assert hashlib.sha256(str([len(c) for c in chunk_arr]).encode()).hexdigest() == expected_hash, \"error on chunk_arr sizes\"\n",
    "\n",
    "assert df9.loc[df9['Country']=='World', 'Gender difference'].values[0] == 7.3, \"df9 data is wrong\"\n",
    "assert df9.dtypes['Country'] == object, \"df9 structure is wrong\"\n",
    "assert df9.dtypes['Gender difference'] == float, \"df9 structure is wrong\" \n",
    "\n",
    "expected_hash = 'c4d7d45402099baaea8a079580a4ef950016ef7403178f72e64ceb3738e4471d'\n",
    "assert hashlib.sha256(str(lit_avg).encode()).hexdigest() == expected_hash, \"lit_avg is wrong\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e990312f7cddd0b2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q10: Calculate average values of `Literacy All` using chunks and avoiding a complete data frame in memory.\n",
    "\n",
    "Using chunks, read file **data/exercises/world_percentage_of_literacy.tsv**, avoid incomplete rows and calculate the average of ***Literacy rate (all)*** without loading all data simultaneously. \n",
    "\n",
    "Use a similar approach of the previous question but don't create any dataframe neither any list with chunks; ***Hint: Use the average definition***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-57fd7700885246eb",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ana/.virtualenvs/blu01/lib/python3.7/site-packages/ipykernel_launcher.py:15: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/ana/.virtualenvs/blu01/lib/python3.7/site-packages/ipykernel_launcher.py:15: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/ana/.virtualenvs/blu01/lib/python3.7/site-packages/ipykernel_launcher.py:15: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/ana/.virtualenvs/blu01/lib/python3.7/site-packages/ipykernel_launcher.py:15: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/ana/.virtualenvs/blu01/lib/python3.7/site-packages/ipykernel_launcher.py:15: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/ana/.virtualenvs/blu01/lib/python3.7/site-packages/ipykernel_launcher.py:15: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/ana/.virtualenvs/blu01/lib/python3.7/site-packages/ipykernel_launcher.py:15: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/ana/.virtualenvs/blu01/lib/python3.7/site-packages/ipykernel_launcher.py:15: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/ana/.virtualenvs/blu01/lib/python3.7/site-packages/ipykernel_launcher.py:15: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/ana/.virtualenvs/blu01/lib/python3.7/site-packages/ipykernel_launcher.py:15: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# Read file data/exercises/world_percentage_of_literacy.tsv\n",
    "# the final average should be in the variable final_avg\n",
    "# You should increment 2 variables in each chunk and use them at the end to calculate final_avg. call them lit_a, lit_b\n",
    "\n",
    "filepath = \"data/exercises/world_percentage_of_literacy.tsv\"\n",
    "\n",
    "chunks_iter = pd.read_csv(\n",
    "    filepath,\n",
    "    sep=\"\\t\",\n",
    "    chunksize=20\n",
    ")\n",
    "\n",
    "def filter_data(data):\n",
    "    data.dropna(inplace=True)\n",
    "    data[\"Literacy rate (all)\"] = data[\"Literacy rate (all)\"].str.replace(\"\\%(.*)\", '').astype(\"float\")\n",
    "    return data\n",
    "\n",
    "lit_a = 0\n",
    "lit_b = 0\n",
    "for data_chunk in chunks_iter:\n",
    "    data_chunk_filtered = filter_data(data_chunk)\n",
    "    lit_a += np.sum(data_chunk_filtered[\"Literacy rate (all)\"])\n",
    "    lit_b += data_chunk_filtered.shape[0]\n",
    "    \n",
    "final_avg = lit_a/lit_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0edcf1260d7886c9",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert math.isclose(83.24370860927154, final_avg, rel_tol=1e-1), \"final_avg is wrong\"\n",
    "assert lit_b==151 or lit_a==151, \"lit_a or lit_b is wrong\"\n",
    "assert int(lit_b) == 12569 or int(lit_a)==12569, \"lit_a or lit_b is wrong\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
