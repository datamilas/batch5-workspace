{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e73ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e3aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#check the data size and distribution of classes:\n",
    "def get_data_stats(X, y):\n",
    "    print(f\"Size of dataset: {len(X)}\")\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    print(f\"Distribution of classes: {dict(zip(unique, counts))}\")\n",
    "\n",
    "get_data_stats(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ce54df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dev test split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_temp, y_temp, test_size=0.5)\n",
    "print(f\"Train size: {len(X_train)}\\nDev size: {len(X_dev)}\\nTest size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43478110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e86b00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4025f7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##BLU07 LN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dfc6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6826db09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    # remove html tags\n",
    "    doc = re.sub(\"<[^>]*>\", \"\", doc)\n",
    "    # lowercase\n",
    "    doc = doc.lower()\n",
    "    # tokenize\n",
    "    words = tokenizer.tokenize(doc)\n",
    "    # remove punctuation\n",
    "    words = [word for word in words if word not in string.punctuation]\n",
    "    # stem\n",
    "    stems = [stemmer.stem(word) for word in words]\n",
    "    new_doc = \" \".join(stems)\n",
    "    return new_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60130f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = docs.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dfbdae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb91d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLU07 LN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0324af98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer to implement sentence cleaning\n",
    "class TextCleanerTransformer(TransformerMixin):\n",
    "    def __init__(self, tokenizer, stemmer, regex_list,\n",
    "                 lower=True, remove_punct=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stemmer = stemmer\n",
    "        self.regex_list = regex_list\n",
    "        self.lower = lower\n",
    "        self.remove_punct = remove_punct\n",
    "        \n",
    "    def transform(self, X, *_):\n",
    "        X = list(map(self._clean_sentence, X))\n",
    "        return X\n",
    "    \n",
    "    def _clean_sentence(self, sentence):\n",
    "        \n",
    "        # Replace given regexes\n",
    "        for regex in self.regex_list:\n",
    "            sentence = re.sub(regex[0], regex[1], sentence)\n",
    "            \n",
    "        # lowercase\n",
    "        if self.lower:\n",
    "            sentence = sentence.lower()\n",
    "\n",
    "        # Split sentence into list of words\n",
    "        words = self.tokenizer.tokenize(sentence)\n",
    "            \n",
    "        # Remove punctuation\n",
    "        if self.remove_punct:\n",
    "            words = list(filter(lambda x: x not in string.punctuation, words))\n",
    "\n",
    "        # Stem words\n",
    "        if self.stemmer:\n",
    "            words = map(self.stemmer.stem, words)\n",
    "\n",
    "        # Join list elements into string\n",
    "        sentence = \" \".join(words)\n",
    "        \n",
    "        return sentence\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11f1902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a tokenizer and a stemmer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "regex_list = [(\"<[^>]*>\", \"\")\n",
    "             ]\n",
    "\n",
    "cleaner = TextCleanerTransformer(tokenizer, stemmer, regex_list)\n",
    "docs = cleaner.transform(train_df.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fe1c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83441bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLU08 Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2fb9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_pca_svm(X_train, y_train, X_test, y_test, num_features=100, seed=42):\n",
    "    \"\"\"Returns a fitted TfidfVectorizer, the truncated svd used, a support vector classifier\n",
    "    and the test predictions computed with these\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (Series): Text data for training\n",
    "        y_train (Series): Labels corresponding to X_train\n",
    "        X_test (Series): Text data for testing\n",
    "        y_test (Series): Labels corresponding to X_test\n",
    "        num_features (int): maximum number of features to use\n",
    "        seed (int): Seed to use for random state\n",
    "\n",
    "    Returns:\n",
    "        vectorizer (CountVectorizer): CountVectorizer, fitted to X_train\n",
    "        pca (PCA): PCA with provided number of features as components\n",
    "        clf (SVC): SVC classifier fitted to the feature-selected training data\n",
    "        y_pred (Series): The predictions computed with our classifier\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(max_features=5000)\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    \n",
    "    dense_X_train = X_train_vec.toarray()\n",
    "    dense_X_test = X_test_vec.toarray()\n",
    "    data_var = np.var(dense_X_train, axis=0).sum()\n",
    "\n",
    "    pca = PCA(n_components=num_features, random_state=seed)\n",
    "    pca.fit(dense_X_train)\n",
    "    X_train_pca = pca.transform(dense_X_train)\n",
    "    X_test_pca = pca.transform(dense_X_test)\n",
    "    \n",
    "    clf =  SVC()\n",
    "    clf.fit(X_train_pca, y_train)\n",
    "    y_pred = clf.predict(X_test_pca)\n",
    "    \n",
    "    explained_variance = 1.0*np.var(X_train_pca, axis=0).sum() / data_var\n",
    "    \n",
    "    return vectorizer, pca, clf, y_pred, explained_variance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
